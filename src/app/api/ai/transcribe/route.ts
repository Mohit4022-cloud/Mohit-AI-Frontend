/**
 * AI-Powered Transcription API Route
 *
 * Handles audio transcription with speaker diarization and sentiment analysis
 * Currently returns mock data for development/testing
 */

import { NextRequest, NextResponse } from "next/server";
import type { TranscriptSegment } from "@/types/transcript";

/**
 * Mock transcription function for development
 * Returns realistic transcript segments with sentiment analysis
 */
async function mockTranscribe(audioUrl: string): Promise<TranscriptSegment[]> {
  // Simulate processing delay
  await new Promise((resolve) => setTimeout(resolve, 1000));

  // Return mock transcript segments with timestamps in milliseconds
  return [
    {
      speaker: "agent",
      text: "Hi, this is John from Mohit AI. I'm calling to discuss how we can help improve your sales team's productivity.",
      startTime: 0,
      endTime: 5000,
      sentiment: {
        score: 0.8,
        magnitude: 0.7,
        label: "positive",
      },
    },
    {
      speaker: "customer",
      text: "Oh, hi John. We're actually looking at solutions for our SDR team. What makes Mohit AI different?",
      startTime: 5000,
      endTime: 10000,
      sentiment: {
        score: 0.6,
        magnitude: 0.5,
        label: "positive",
      },
    },
    {
      speaker: "agent",
      text: "Great question! Mohit AI uses real-time AI coaching to help SDRs improve during calls, not just after. We provide live sentiment analysis and conversation insights.",
      startTime: 10000,
      endTime: 17000,
      sentiment: {
        score: 0.9,
        magnitude: 0.8,
        label: "positive",
      },
    },
    {
      speaker: "customer",
      text: "That sounds interesting. How accurate is the sentiment analysis? And does it work with our existing phone system?",
      startTime: 17000,
      endTime: 22000,
      sentiment: {
        score: 0.5,
        magnitude: 0.6,
        label: "positive",
      },
    },
    {
      speaker: "agent",
      text: "Our sentiment analysis is over 90% accurate and continuously improves. We integrate with most major phone systems including RingCentral, Aircall, and Twilio. The setup usually takes less than 30 minutes.",
      startTime: 22000,
      endTime: 30000,
      sentiment: {
        score: 0.85,
        magnitude: 0.75,
        label: "positive",
      },
    },
    {
      speaker: "customer",
      text: "What about pricing? We have a team of 15 SDRs.",
      startTime: 30000,
      endTime: 33000,
      sentiment: {
        score: 0.3,
        magnitude: 0.4,
        label: "neutral",
      },
    },
    {
      speaker: "agent",
      text: "For a team of 15, we offer volume pricing that typically comes out to less than the cost of one lost deal per month. I'd be happy to put together a custom quote. Would you be interested in seeing a quick demo first?",
      startTime: 33000,
      endTime: 41000,
      sentiment: {
        score: 0.75,
        magnitude: 0.7,
        label: "positive",
      },
    },
  ];
}

/**
 * POST /api/ai/transcribe
 *
 * Transcribes audio and returns segments with sentiment analysis
 *
 * Request body:
 * - audioUrl: URL of the audio file to transcribe
 * - config: Optional configuration (model, language, etc.)
 *
 * @returns Transcript segments with speaker identification and sentiment
 */
export async function POST(request: NextRequest) {
  try {
    // Verify authentication
    const authHeader = request.headers.get("authorization");
    if (!authHeader || !authHeader.startsWith("Bearer ")) {
      return NextResponse.json({ error: "Unauthorized" }, { status: 401 });
    }

    const body = await request.json();
    const { audioUrl, config } = body;

    if (!audioUrl) {
      return NextResponse.json(
        { error: "Audio URL is required" },
        { status: 400 },
      );
    }

    // In development or if OpenAI is not configured, use mock transcription
    if (
      !process.env["OPENAI_API_KEY"] ||
      process.env.NODE_ENV === "development"
    ) {
      const segments = await mockTranscribe(audioUrl);
      return NextResponse.json({
        segments,
        metadata: {
          duration: 41000,
          language: config?.language || "en-US",
          confidence: 0.95,
        },
      });
    }

    // Production implementation would go here:
    // 1. Download audio from URL
    // 2. Send to OpenAI Whisper API
    // 3. Process response with speaker diarization
    // 4. Add sentiment analysis
    // 5. Return formatted segments

    // For now, return mock data
    const segments = await mockTranscribe(audioUrl);
    return NextResponse.json({
      segments,
      metadata: {
        duration: 41000,
        language: config?.language || "en-US",
        confidence: 0.95,
      },
    });
  } catch (error) {
    console.error("Transcription error:", error);
    return NextResponse.json(
      { error: "Transcription failed" },
      { status: 500 },
    );
  }
}

/**
 * GET /api/ai/transcribe
 *
 * Returns sample transcript data for testing
 */
export async function GET(request: NextRequest) {
  const segments = await mockTranscribe("sample");
  return NextResponse.json(
    {
      segments,
      metadata: {
        duration: 41000,
        language: "en-US",
        confidence: 0.95,
      },
    },
    {
      headers: {
        "Content-Type": "application/json",
        "Cache-Control": "no-store",
      },
    },
  );
}
